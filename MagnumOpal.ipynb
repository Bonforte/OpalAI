{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import timedelta, datetime\n",
    "import mplfinance as mpf\n",
    "import pandas as pd\n",
    "import time\n",
    "from numpy import trapz,nan\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# for modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import to_categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ichimoku(df_stock):\n",
    "    #Tenkan Sen\n",
    "    tenkan_max = df_stock['High'].rolling(window = 9, min_periods = 0).max()\n",
    "    tenkan_min = df_stock['Low'].rolling(window = 9, min_periods = 0).min()\n",
    "    df_stock['tenkan_avg'] = (tenkan_max + tenkan_min) / 2\n",
    "\n",
    "    #Kijun Sen\n",
    "    kijun_max = df_stock['High'].rolling(window = 26, min_periods = 0).max()\n",
    "    kijun_min = df_stock['Low'].rolling(window = 26, min_periods = 0).min()\n",
    "    df_stock['kijun_avg'] = (kijun_max + kijun_min) / 2\n",
    "\n",
    "    df_stock['senkou_a'] = ((df_stock['kijun_avg'] + df_stock['tenkan_avg']) / 2).shift(26)\n",
    "\n",
    "    #Senkou Span B\n",
    "    #52 period High + Low / 2\n",
    "    senkou_b_max = df_stock['High'].rolling(window = 52, min_periods = 0).max()\n",
    "    senkou_b_min = df_stock['Low'].rolling(window = 52, min_periods = 0).min()\n",
    "    df_stock['senkou_b'] = ((senkou_b_max + senkou_b_min) / 2).shift(52)\n",
    "\n",
    "    #Chikou Span\n",
    "    #Current close shifted -26\n",
    "    df_stock['chikou'] = (df_stock['Close']).shift(-26)\n",
    "\n",
    "    return df_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adx(high, low, close, lookback):\n",
    "    plus_dm = high.diff()\n",
    "    minus_dm = low.diff()\n",
    "    plus_dm[plus_dm < 0] = 0\n",
    "    minus_dm[minus_dm > 0] = 0\n",
    "    \n",
    "    tr1 = pd.DataFrame(high - low)\n",
    "    tr2 = pd.DataFrame(abs(high - close.shift(1)))\n",
    "    tr3 = pd.DataFrame(abs(low - close.shift(1)))\n",
    "    frames = [tr1, tr2, tr3]\n",
    "    tr = pd.concat(frames, axis = 1, join = 'inner').max(axis = 1)\n",
    "    atr = tr.rolling(lookback).mean()\n",
    "    \n",
    "    plus_di = 100 * (plus_dm.ewm(alpha = 1/lookback).mean() / atr)\n",
    "    minus_di = abs(100 * (minus_dm.ewm(alpha = 1/lookback).mean() / atr))\n",
    "    dx = (abs(plus_di - minus_di) / abs(plus_di + minus_di)) * 100\n",
    "    adx = ((dx.shift(1) * (lookback - 1)) + dx) / lookback\n",
    "    adx_smooth = adx.ewm(alpha = 1/lookback).mean()\n",
    "    return plus_di, minus_di, adx_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = ['APA','AMD', 'ADSK', 'NFLX', 'RACE', 'CDNS', 'NXPI', 'QCOM', 'CVX', 'ED', 'FANG', 'FSLR', 'HES', 'MPC', 'NVDA', 'OMC', 'SLB', 'TEAM', 'VLO']\n",
    "#stocks = ['NFLX', 'RACE', 'MPC', 'SLB']\n",
    "df_stocks = dict()\n",
    "\n",
    "for stock in stocks:\n",
    "    df_stocks[stock] = yf.Ticker(stock).history('2y')\n",
    "\n",
    "    df_stocks[stock]['Date'] = df_stocks[stock].index.values\n",
    "    df_stocks[stock] = compute_ichimoku(df_stocks[stock])\n",
    "    df_stocks[stock] = df_stocks[stock].drop(['chikou', 'Dividends', 'Stock Splits'], axis=1).dropna()\n",
    "    df_stocks[stock]['cloud_avg_dist'] = (df_stocks[stock]['senkou_a'] + df_stocks[stock]['senkou_b']) / 2 - df_stocks[stock]['Close']\n",
    "    df_stocks[stock]['base_lines_avg_dist'] = (df_stocks[stock]['tenkan_avg'] + df_stocks[stock]['kijun_avg']) / 2 - df_stocks[stock]['Close']\n",
    "    df_stocks[stock]['kijun_dist'] = df_stocks[stock]['kijun_avg'] - df_stocks[stock]['Close']\n",
    "    df_stocks[stock]['tenkan_dist'] = df_stocks[stock]['tenkan_avg'] - df_stocks[stock]['Close']\n",
    "    df_stocks[stock]['senkou_a_dist'] = df_stocks[stock]['senkou_a'] - df_stocks[stock]['Close']\n",
    "    df_stocks[stock]['senkou_b_dist'] = df_stocks[stock]['senkou_b'] - df_stocks[stock]['Close']\n",
    "    df_stocks[stock]['senkou_range'] = df_stocks[stock]['senkou_a'] - df_stocks[stock]['senkou_b']\n",
    "    df_stocks[stock]['baseline_range'] = df_stocks[stock]['tenkan_avg'] - df_stocks[stock]['kijun_avg']\n",
    "    df_stocks[stock]['high_low_difference'] = df_stocks[stock]['High'] - df_stocks[stock]['Low']\n",
    "    df_stocks[stock]['adx'] = pd.DataFrame(get_adx(df_stocks[stock]['High'], df_stocks[stock]['Low'], df_stocks[stock]['Close'], 14)[2]).rename(columns = {0:'adx'})\n",
    "    df_stocks[stock]['surface'] = nan\n",
    "    df_stocks[stock]['slope_tenkan'] = nan\n",
    "    df_stocks[stock]['slope_kijun'] = nan\n",
    "\n",
    "#print(df_stocks['IPAR'].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_intervals = dict()\n",
    "for stock in stocks:\n",
    "    first_entry = df_stocks[stock].head(1)\n",
    "    last_order = first_entry['tenkan_avg'] < first_entry['kijun_avg']\n",
    "    border_dates = []\n",
    "    for index, row in df_stocks[stock].iterrows():\n",
    "        new_order = row['tenkan_avg'] < row['kijun_avg']\n",
    "        if new_order is not last_order:\n",
    "            border_dates.append(row['Date'])\n",
    "\n",
    "        last_sell_line = row['tenkan_avg']\n",
    "        last_buy_line = row['kijun_avg']\n",
    "        last_order = last_sell_line < last_buy_line\n",
    "\n",
    "    border_dates.append(df_stocks[stock]['Date'].iloc[-1])\n",
    "    border_dates.pop(0)\n",
    "\n",
    "\n",
    "    date_intervals[stock] = []\n",
    "    for index in range(len(border_dates)-1):\n",
    "        date_intervals[stock].append((border_dates[index], border_dates[index+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock in stocks: \n",
    "    for interval in date_intervals[stock]:\n",
    "        interval_df_stock = df_stocks[stock][(df_stocks[stock]['Date'] >= interval[0]) & (df_stocks[stock]['Date'] <= interval[1])]\n",
    "        for index, entry in interval_df_stock.iterrows():\n",
    "            entry_interval_df_stock = interval_df_stock[interval_df_stock['Date'] <= entry['Date']]\n",
    "            interval_buy_line = entry_interval_df_stock['tenkan_avg']\n",
    "            interval_sell_line = entry_interval_df_stock['kijun_avg']\n",
    "\n",
    "            buy_area_interval = trapz(interval_buy_line, dx=1)\n",
    "            sell_area_interval = trapz(interval_sell_line, dx=1)\n",
    "\n",
    "            area_between_lines_interval = buy_area_interval - sell_area_interval\n",
    "            df_stocks[stock].at[index,'surface'] = area_between_lines_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock in stocks:    \n",
    "    for index, entry in df_stocks[stock].iterrows():\n",
    "        current_date = entry['Date']\n",
    "        intermediary_df_stock = df_stocks[stock][df_stocks[stock]['Date']<=current_date]\n",
    "        slope_sell = None\n",
    "        slope_buy = None\n",
    "        interval_buy_line = intermediary_df_stock['tenkan_avg']\n",
    "        interval_sell_line = intermediary_df_stock['kijun_avg']\n",
    "        if len(interval_sell_line) > 2 and len(interval_buy_line) > 2:\n",
    "            x1_sell,y1_sell = 1, interval_sell_line[-2]\n",
    "            x2_sell,y2_sell = 2, interval_sell_line[-1]\n",
    "            slope_sell = ((y2_sell-y1_sell)/(x2_sell-x1_sell))\n",
    "            x1_buy,y1_buy = 1, interval_buy_line[-2]\n",
    "            x2_buy,y2_buy = 2, interval_buy_line[-1]\n",
    "            slope_buy = ((y2_buy-y1_buy)/(x2_buy-x1_buy))\n",
    "\n",
    "        df_stocks[stock].at[index,'slope_tenkan'] = slope_buy\n",
    "        df_stocks[stock].at[index,'slope_kijun'] = slope_sell\n",
    "    df_stocks[stock] = df_stocks[stock].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_check_interval = 30\n",
    "\n",
    "for stock in stocks:\n",
    "    start_index = 0\n",
    "    df_stocks[stock]['signal'] = 0\n",
    "    while(start_index < len(df_stocks[stock])):\n",
    "        df_stock_filtered_interval = df_stocks[stock].iloc[start_index:start_index + window_check_interval]\n",
    "        minimum_price = min(df_stock_filtered_interval['Close'])\n",
    "        maximum_price = max(df_stock_filtered_interval['Close'])\n",
    "        if maximum_price >= 1.1 * minimum_price:\n",
    "            maximum_date = df_stock_filtered_interval.loc[df_stock_filtered_interval['Close'] == maximum_price, 'Date'].values[0]\n",
    "            minimum_date = df_stock_filtered_interval.loc[df_stock_filtered_interval['Close'] == minimum_price, 'Date'].values[0]\n",
    "            \n",
    "\n",
    "            if maximum_date > minimum_date:\n",
    "                df_stocks[stock].loc[(df_stocks[stock]['Date'] >= minimum_date) & (df_stocks[stock]['Date'] < maximum_date), 'signal'] = 1\n",
    "            \n",
    "\n",
    "        start_index += window_check_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Date    Volume       adx  \\\n",
      "Date                                                                \n",
      "2021-10-07 00:00:00-04:00 2021-10-07 04:00:00  0.079164  0.792192   \n",
      "2021-10-08 00:00:00-04:00 2021-10-08 04:00:00  0.003565  0.514136   \n",
      "2021-10-11 00:00:00-04:00 2021-10-11 04:00:00  0.012244  0.457060   \n",
      "2021-10-12 00:00:00-04:00 2021-10-12 04:00:00  0.011346  0.460123   \n",
      "2021-10-13 00:00:00-04:00 2021-10-13 04:00:00  0.024596  0.461955   \n",
      "...                                       ...       ...       ...   \n",
      "2023-06-27 00:00:00-04:00 2023-06-27 04:00:00  0.226024  0.743774   \n",
      "2023-06-28 00:00:00-04:00 2023-06-28 04:00:00  0.312215  0.706480   \n",
      "2023-06-29 00:00:00-04:00 2023-06-29 04:00:00  0.167596  0.671850   \n",
      "2023-06-30 00:00:00-04:00 2023-06-30 04:00:00  0.253470  0.640584   \n",
      "2023-07-03 00:00:00-04:00 2023-07-03 04:00:00  0.037158  0.623429   \n",
      "\n",
      "                           cloud_avg_dist  base_lines_avg_dist  senkou_range  \\\n",
      "Date                                                                           \n",
      "2021-10-07 00:00:00-04:00        0.619493             0.587939      0.496677   \n",
      "2021-10-08 00:00:00-04:00        0.633249             0.604255      0.514443   \n",
      "2021-10-11 00:00:00-04:00        0.642459             0.616491      0.529601   \n",
      "2021-10-12 00:00:00-04:00        0.645858             0.618518      0.538320   \n",
      "2021-10-13 00:00:00-04:00        0.637295             0.589171      0.541108   \n",
      "...                                   ...                  ...           ...   \n",
      "2023-06-27 00:00:00-04:00        0.112716             0.335295      0.764277   \n",
      "2023-06-28 00:00:00-04:00        0.139320             0.418517      0.764277   \n",
      "2023-06-29 00:00:00-04:00        0.149682             0.450863      0.764351   \n",
      "2023-06-30 00:00:00-04:00        0.103597             0.288586      0.744623   \n",
      "2023-07-03 00:00:00-04:00        0.107766             0.453311      0.717164   \n",
      "\n",
      "                           baseline_range  high_low_difference  kijun_dist  \\\n",
      "Date                                                                         \n",
      "2021-10-07 00:00:00-04:00        0.387544             0.020362    0.609697   \n",
      "2021-10-08 00:00:00-04:00        0.367303             0.040724    0.628299   \n",
      "2021-10-11 00:00:00-04:00        0.362136             0.071141    0.638667   \n",
      "2021-10-12 00:00:00-04:00        0.363308             0.065360    0.639659   \n",
      "2021-10-13 00:00:00-04:00        0.363308             0.002011    0.619227   \n",
      "...                                   ...                  ...         ...   \n",
      "2023-06-27 00:00:00-04:00        1.000000             0.307940    0.214659   \n",
      "2023-06-28 00:00:00-04:00        1.000000             0.266407    0.272600   \n",
      "2023-06-29 00:00:00-04:00        1.000000             0.184095    0.295120   \n",
      "2023-06-30 00:00:00-04:00        1.000000             0.196429    0.182138   \n",
      "2023-07-03 00:00:00-04:00        0.615790             0.107574    0.434298   \n",
      "\n",
      "                           tenkan_dist  senkou_a_dist  senkou_b_dist  \\\n",
      "Date                                                                   \n",
      "2021-10-07 00:00:00-04:00     0.545789       0.608159       0.622016   \n",
      "2021-10-08 00:00:00-04:00     0.552491       0.629185       0.629713   \n",
      "2021-10-11 00:00:00-04:00     0.563349       0.644324       0.634002   \n",
      "2021-10-12 00:00:00-04:00     0.566328       0.650928       0.634759   \n",
      "2021-10-13 00:00:00-04:00     0.533069       0.642344       0.626306   \n",
      "...                                ...            ...            ...   \n",
      "2023-06-27 00:00:00-04:00     0.616183       0.133535       0.094572   \n",
      "2023-06-28 00:00:00-04:00     0.710503       0.163021       0.118545   \n",
      "2023-06-29 00:00:00-04:00     0.747162       0.174530       0.127863   \n",
      "2023-06-30 00:00:00-04:00     0.563245       0.117034       0.091553   \n",
      "2023-07-03 00:00:00-04:00     0.526151       0.112722       0.102573   \n",
      "\n",
      "                            surface  slope_tenkan  slope_kijun  signal  \n",
      "Date                                                                    \n",
      "2021-10-07 00:00:00-04:00  0.274652      0.268347     0.282794       0  \n",
      "2021-10-08 00:00:00-04:00  0.272268      0.265643     0.282794       0  \n",
      "2021-10-11 00:00:00-04:00  0.269471      0.291151     0.282794       0  \n",
      "2021-10-12 00:00:00-04:00  0.266609      0.299894     0.280735       1  \n",
      "2021-10-13 00:00:00-04:00  0.263766      0.299894     0.282794       1  \n",
      "...                             ...           ...          ...     ...  \n",
      "2023-06-27 00:00:00-04:00  0.934841      0.332385     0.282794       0  \n",
      "2023-06-28 00:00:00-04:00  0.952691      0.299894     0.282794       0  \n",
      "2023-06-29 00:00:00-04:00  0.970542      0.299894     0.282794       0  \n",
      "2023-06-30 00:00:00-04:00  0.988393      0.299894     0.282794       0  \n",
      "2023-07-03 00:00:00-04:00  1.000000      0.266049     0.922862       0  \n",
      "\n",
      "[436 rows x 16 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
      "C:\\Users\\gnite\\AppData\\Local\\Temp\\ipykernel_26692\\569221040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n"
     ]
    }
   ],
   "source": [
    "df_stocks_filtered = dict()\n",
    "ncn = ['Volume', 'cloud_avg_dist', 'adx', 'base_lines_avg_dist', 'kijun_dist', 'senkou_range', 'baseline_range', 'high_low_difference', 'tenkan_dist', 'senkou_a_dist', 'senkou_b_dist', 'surface', 'slope_tenkan', 'slope_kijun']\n",
    "\n",
    "for stock in stocks:\n",
    "    df_stocks_filtered[stock] = df_stocks[stock][['Date', 'Volume', 'adx', 'cloud_avg_dist', 'base_lines_avg_dist', 'senkou_range', 'baseline_range', 'high_low_difference', 'kijun_dist', 'tenkan_dist', 'senkou_a_dist', 'senkou_b_dist', 'surface', 'slope_tenkan', 'slope_kijun', 'signal']]\n",
    "\n",
    "    df_stocks_filtered[stock][ncn] = (df_stocks_filtered[stock][ncn] - df_stocks_filtered[stock][ncn].min())/(df_stocks_filtered[stock][ncn].max()-df_stocks_filtered[stock][ncn].min())\n",
    "\n",
    "print(df_stocks_filtered['NVDA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               3840      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,122\n",
      "Trainable params: 45,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/90\n",
      "30/30 [==============================] - 1s 6ms/step - loss: 0.6332 - accuracy: 0.7100 - val_loss: 0.5710 - val_accuracy: 0.7467\n",
      "Epoch 2/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.5838 - accuracy: 0.7100 - val_loss: 0.5453 - val_accuracy: 0.7467\n",
      "Epoch 3/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.5509 - accuracy: 0.7100 - val_loss: 0.5262 - val_accuracy: 0.7467\n",
      "Epoch 4/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7100 - val_loss: 0.5032 - val_accuracy: 0.7333\n",
      "Epoch 5/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7800 - val_loss: 0.4954 - val_accuracy: 0.7200\n",
      "Epoch 6/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.4709 - accuracy: 0.7900 - val_loss: 0.4976 - val_accuracy: 0.7067\n",
      "Epoch 7/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.4481 - accuracy: 0.7900 - val_loss: 0.5090 - val_accuracy: 0.6933\n",
      "Epoch 8/90\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.4313 - accuracy: 0.8000 - val_loss: 0.5224 - val_accuracy: 0.6933\n",
      "Epoch 9/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.4201 - accuracy: 0.7933 - val_loss: 0.5454 - val_accuracy: 0.6800\n",
      "Epoch 10/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.4054 - accuracy: 0.8033 - val_loss: 0.5689 - val_accuracy: 0.6933\n",
      "Epoch 11/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8000 - val_loss: 0.6008 - val_accuracy: 0.6800\n",
      "Epoch 12/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3824 - accuracy: 0.8067 - val_loss: 0.6363 - val_accuracy: 0.6667\n",
      "Epoch 13/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.8067 - val_loss: 0.6719 - val_accuracy: 0.6667\n",
      "Epoch 14/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3597 - accuracy: 0.8100 - val_loss: 0.6897 - val_accuracy: 0.6400\n",
      "Epoch 15/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3460 - accuracy: 0.8067 - val_loss: 0.7381 - val_accuracy: 0.6400\n",
      "Epoch 16/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8167 - val_loss: 0.7646 - val_accuracy: 0.6267\n",
      "Epoch 17/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8167 - val_loss: 0.8027 - val_accuracy: 0.6533\n",
      "Epoch 18/90\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.3045 - accuracy: 0.8300 - val_loss: 0.8471 - val_accuracy: 0.6533\n",
      "Epoch 19/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2952 - accuracy: 0.8367 - val_loss: 0.8879 - val_accuracy: 0.6267\n",
      "Epoch 20/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2841 - accuracy: 0.8367 - val_loss: 0.9434 - val_accuracy: 0.6533\n",
      "Epoch 21/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2752 - accuracy: 0.8533 - val_loss: 1.0188 - val_accuracy: 0.6533\n",
      "Epoch 22/90\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2608 - accuracy: 0.8700 - val_loss: 1.0047 - val_accuracy: 0.6267\n",
      "Epoch 23/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2515 - accuracy: 0.8700 - val_loss: 1.1310 - val_accuracy: 0.6533\n",
      "Epoch 24/90\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2330 - accuracy: 0.8967 - val_loss: 1.1490 - val_accuracy: 0.6267\n",
      "Epoch 25/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2318 - accuracy: 0.8933 - val_loss: 1.2248 - val_accuracy: 0.6267\n",
      "Epoch 26/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2186 - accuracy: 0.8967 - val_loss: 1.4131 - val_accuracy: 0.6667\n",
      "Epoch 27/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2079 - accuracy: 0.9100 - val_loss: 1.2669 - val_accuracy: 0.6133\n",
      "Epoch 28/90\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1892 - accuracy: 0.9167 - val_loss: 1.2781 - val_accuracy: 0.6133\n",
      "Epoch 29/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1754 - accuracy: 0.9367 - val_loss: 1.5623 - val_accuracy: 0.6133\n",
      "Epoch 30/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1540 - accuracy: 0.9467 - val_loss: 1.8428 - val_accuracy: 0.6667\n",
      "Epoch 31/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1481 - accuracy: 0.9533 - val_loss: 1.1701 - val_accuracy: 0.6133\n",
      "Epoch 32/90\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9133 - val_loss: 1.8985 - val_accuracy: 0.7467\n",
      "Epoch 33/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2007 - accuracy: 0.9133 - val_loss: 2.3975 - val_accuracy: 0.6533\n",
      "Epoch 34/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1510 - accuracy: 0.9300 - val_loss: 2.0663 - val_accuracy: 0.7867\n",
      "Epoch 35/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2020 - accuracy: 0.9133 - val_loss: 1.4836 - val_accuracy: 0.4933\n",
      "Epoch 36/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.4087 - accuracy: 0.8167 - val_loss: 1.1707 - val_accuracy: 0.6133\n",
      "Epoch 37/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1973 - accuracy: 0.9200 - val_loss: 1.7783 - val_accuracy: 0.5867\n",
      "Epoch 38/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2135 - accuracy: 0.9067 - val_loss: 2.0424 - val_accuracy: 0.6800\n",
      "Epoch 39/90\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3127 - accuracy: 0.8433 - val_loss: 1.4682 - val_accuracy: 0.6133\n",
      "Epoch 40/90\n",
      " 1/30 [>.............................] - ETA: 0s - loss: 0.3205 - accuracy: 0.8000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 28\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[39m# compile the model\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     25\u001b[0m                 loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     26\u001b[0m                 metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 28\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     29\u001b[0m                     X_train,\n\u001b[0;32m     30\u001b[0m                     Y_train,\n\u001b[0;32m     31\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m90\u001b[39;49m, \u001b[39m# you can set this to a big number!\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m     33\u001b[0m                     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[0;32m     34\u001b[0m                     shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     35\u001b[0m                     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[0;32m     36\u001b[0m                     )\n\u001b[0;32m     38\u001b[0m     predictions[stock] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X[\u001b[39m-\u001b[39m\u001b[39m30\u001b[39m:])[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m predictions\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opal_grade = dict()\n",
    "for stock in stocks:\n",
    "    opal_grade[stock] = []\n",
    "for i in range(6):\n",
    "    predictions = dict()\n",
    "    for stock in stocks:\n",
    "        print(stock)\n",
    "        Y = df_stocks_filtered[stock]['signal']\n",
    "        X = df_stocks_filtered[stock].drop(['signal', 'Date'], axis=1)\n",
    "\n",
    "        Y_train = Y[:-30]\n",
    "        X_train = X[:-30]\n",
    "        Y_train = to_categorical(Y_train)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_shape=(X_train.shape[1],), activation='relu')) # Add an input shape! (features,)\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        #model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(2, activation='sigmoid'))\n",
    "        model.summary() \n",
    "\n",
    "        # compile the model\n",
    "        model.compile(optimizer='Adam', \n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "        \n",
    "        history = model.fit(\n",
    "                        X_train,\n",
    "                        Y_train,\n",
    "                        epochs=90, # you can set this to a big number!\n",
    "                        batch_size=10,\n",
    "                        validation_split=0.2,\n",
    "                        shuffle=False,\n",
    "                        verbose=1\n",
    "                        )\n",
    "        \n",
    "        predictions[stock] = model.predict(X[-30:])[-1]\n",
    "    \n",
    "\n",
    "    for key, value in predictions.items():\n",
    "        index = predictions[key].argmax()\n",
    "        opal_grade[key].append(index*predictions[key][index])\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA - 0.0\n",
      "AMD - 0.8772305647532145\n",
      "ADSK - 9.921391308307648\n",
      "NFLX - 4.143649836381276\n",
      "RACE - 0.0\n",
      "CDNS - 8.102254768212637\n",
      "NXPI - 0.0\n",
      "QCOM - 0.0\n",
      "CVX - 0.0\n",
      "ED - 0.0\n",
      "FANG - 1.6665259997049968\n",
      "FSLR - 0.0\n",
      "HES - 0.0\n",
      "MPC - 2.5513356924057007\n",
      "NVDA - 0.0\n",
      "OMC - 0.0\n",
      "SLB - 0.0\n",
      "TEAM - 0.0\n",
      "VLO - 0.0\n"
     ]
    }
   ],
   "source": [
    "for key, value in opal_grade.items():\n",
    "    print(f'{key} - {10*sum(value)/6}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
